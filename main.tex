\documentclass{article}

\usepackage{color}
\usepackage{xcolor}

\newcommand{\setbackgroundcolour}{\pagecolor[rgb]{0,0.168,0.211}}
\newcommand{\settextcolour}{\color[rgb]{0.512,0.578,0.586}}
\newcommand{\darkmode}{\setbackgroundcolour\settextcolour}
\darkmode

\usepackage{hyperref}

\usepackage{tikz}
\usetikzlibrary{calc}
\include{figs}

\usepackage{booktabs}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\newcommand\given{\,\vert\,}
\DeclarePairedDelimiter\abs{\lvert}{\rvert}

\usepackage[style=authoryear, date=year, natbib=true, maxcitenames=2,
            maxbibnames=10, uniquelist=false]{biblatex}
\setlength\bibitemsep{1.5\itemsep}
\DeclareFieldFormat[article, book, inbook, incollection, inproceedings, misc,
                    thesis, unpublished, techreport, phdthesis]{title}{#1}
\DeclareNameAlias{sortname}{family-given}
\addbibresource{ans-notes.bib}
\renewcommand*{\intitlepunct}{\space}

% TODO: Maybe delete this stuff:
\usepackage{siunitx}
\DeclareSIUnit[parse-numbers=false]\bit{bits}
\newcommand\bits{\,\si\bit}

\usepackage{listings}
\lstset{language=Python, mathescape=true, columns=flexible, xleftmargin=0.9cm,
        basicstyle=\ttfamily, commentstyle=\textrm}

\usepackage{cleveref}
\newtheorem{problem}{Problem}

\newcommand{\push}{\texttt{push}}
\newcommand{\pop}{\texttt{pop}}

\author{James Townsend}
\title{Notes on asymmetric numeral systems}

\begin{document}
\maketitle
\section{Introduction}
We are interested in algorithms for lossless compression of sequences of data.
The range variant of asymmetric numeral systems (ANS) is such an algorithm,
and, like arithmetic coding (AC), it is close to optimal in terms of
compression rate \citep{dudaAsymmetricNumeralSystems2009}. The key difference
between ANS and AC is that ANS is last-in-first-out (LIFO), or `stack-like',
while AC is first-in-first-out (FIFO), or `queue-like'.

ANS comprises two functions, which we denote \push\ and \pop, for encoding and
decoding, respectively (the names refer to the analagous stack operations). The
\push\ function accepts some pre-compressed information $m$ (short for
`message'), and a symbol $x$ to be compressed, and returns a piece of
compressed information, $m'$. Thus it has the signiature
\begin{equation}
  \push:(m, x) \mapsto m'.
\end{equation}
The new compressed state, $m'$, contains precisely the same information as the
pair $(m, x)$, and therefore \push\ can be inverted, giving a decoder mapping.
The decoder, \pop, maps from $m'$ back to $m, x$:
\begin{equation}
  \pop:m' \mapsto (m, x).
\end{equation}
The functions \push\ and \pop\ are inverse to one another, so
$\push(\pop(m))=m$ and $\pop(\push(m, x)) = (m, x)$.

Encoding and decoding both require knowledge of some model over symbols. We use
$\mathcal{A} = \{a_1, \ldots, a_I\}$ to denote the alphabet from which symbols
$x$ are drawn. We denote the probability mass function of the model $P$. Later
we will need to assume that all probability masses are quantized to some
precision $r_p$, i.e.\ that there exist integers $p_1, \ldots, p_I$ such that
$P(a_i) = \frac{p_i}{2^{r_p}}$ for each $i = 1,\ldots,I$.

\section{Building an encoder/decoder pair}
The problem which the ANS encoder solves is

\begin{problem}\label{prob:default}
  Given a sequence of random variables $X_1, \ldots, X_N$, find an invertible
  algorithm which will map any sample $x_1, \ldots, x_N$ to a binary message
  $b$, such that the length of $b$ is close to the information content $h(x_1,
  \ldots, x_N)$.
\end{problem}

Given that the algorithm is invertible, we can reformulate \Cref{prob:default}
in terms of the inverse. This leads to a different, but equivalent, problem
statement:

\begin{problem}\label{prob:alt}
  Given a sequence of random variables $X_1, \ldots, X_N$, find an invertible
  algorithm which maps a source of bits to a sequence $x_1, \ldots, x_N$, such
  that the number of bits observed is close to the information content $h(x_1,
  \ldots, x_N)$.
\end{problem}

Our description of the details of ANS focuses on the decoder, because this
leads to a more straightforward presentation. Having described the decoder and
shown that it solves \Cref{prob:alt}, we then show how to invert it to form an
encoder.

The decoding algorithm we describe will be formed from a series of ANS \pop\
operations, its inverse will be formed from ANS \push\ operations.

\subsection*{The structure of the message}
We use a pair $m = (s, t)$ as the data structure for $m$. The element $t$ is a
stack of unsigned integers of some fixed precision $r_t$. This stack has its
own push and pop operations, which we denote $\texttt{stack\_push}$ and
$\texttt{stack\_pop}$ respectively. The element $s$ is an unsigned integer with
precision $r_s$ where $r_s > r_t$. In our implementation we use $r_t = 32$ and
$r_s = 64$. The stack $t$, along with its pop operation, can model the `source
of bits' from our problem statement above.
% TODO: Diagram of $m$

\subsection*{Constructing the pop operation}
Our strategy for performing a decode with \pop\ will be to firstly to extract a
symbol from $s$. We do this using a bijective function $d:\mathbb
N\rightarrow\mathbb N\times\mathcal{A}$, which takes an integer $s$ as input
and returns a pair $(s', x)$, where $s'$ is an integer and $x$ is a symbol.
Thus \pop\ begins
\begin{lstlisting}
def pop($s$, $t$):
    $s'$, $x= d(s)$
\end{lstlisting}
We design the function $d$ so that
\begin{equation}\label{eq:inner-decoder}
  \log s'\geq\log s - h(x) + \log (1 - \epsilon)
\end{equation}
where $\epsilon := 2^{r_p + r_t - r_s}$. We give details of $d$ and prove
\cref{eq:inner-decoder} below. Note that for small $\epsilon$ we have $\log (1
- \epsilon) \approx -\frac{\epsilon}{\ln2}$, and thus this term is small.

After extracting a symbol using $d$, we check whether $s'$ is below a certain
threshold, and if it is we \texttt{stack\_pop} an integer from $t$ and move its
contents into the lower order bits of $s'$, increasing the size of $s'$. We
refer to this as `renormalization'. Having done this, we return the new message
and the symbol $x$. The full definition of \pop\ is thus
\begin{lstlisting}
def pop($s$, $t$):
    $s'$, $x= d(s)$
    $s$, $t =$ renorm($s'$, $t$)
    return ($s$, $t$), $x$
\end{lstlisting}

Renormalization is necessary to ensure that the $s$ returned is large enough
for any future \pop\ operation. The \texttt{renorm} function has a while loop,
which pushes elements from $t$ into the lower order bits of $s$ until $s$ is
full to capacity. To be precise:

\begin{lstlisting}
def renorm($s$, $t$):
    # While $s$ has space for another element from $t$
    while $s < 2^{r_s - r_t}$:
        # Pop an element $t_\mathrm{top}$ from $t$
        $t$, $t_{\mathrm{top}}$ = stack_pop($t$)

        # and push $t_\mathrm{top}$ into the lower bits of $s$
        $s$ = $2^{r_t} \cdot s$ + $t_{\mathrm{top}}$
    return $s$, $t$
\end{lstlisting}

The condition $s < 2^{r_s - r_t}$ guarantees that $2^{r_t} \cdot s +
t_{\text{top}} < 2^{r_s}$, and thus there can be no loss of information
resulting from overflow. We also have
\begin{equation}
  \log (2^{r_t} \cdot s + t_\text{top}) \geq r_t + \log s.
\end{equation}
Applying this repeatedly, once for each iteration of the while loop in
\texttt{renorm}, we have
\begin{equation}\label{eq:renorm}
\log s \geq \log s' + r_t\cdot\left[\text{\# elements popped from $t$}\right]
\end{equation}
where $s, t = \texttt{renorm}(s', t)$ as in the definition of \pop. Plugging
\cref{eq:inner-decoder} into \cref{eq:renorm} gives
\begin{equation}
  \log s_{n+1} \geq \log s_n - h(x_n)
  + b_n + \log (1 -
  \epsilon)
\end{equation}

If we apply $D$ many times to decode a sequence of symbols,
\cref{eq:ans-approx} can be applied recursively. Denote the decoded symbols
$x_1, \ldots, x_N$, $s, t$ the coder state before decoding and $s', t'$ the
state after decoding, then
\begin{equation}
        \log s' + \sum_{n=1}^Nh(x_n) - r_t\cdot\left[\text{\# elements popped
        from $t$}\right] \approx \log s.
\end{equation}
and in particular
\begin{align}
         r_t\cdot\left[\text{\# elements popped
         from $t$}\right] &\approx \sum_{n=1}^Nh(x_n) + \log s' - \log s\\
                          &\leq \sum_{n=1}^Nh(x_n) + r_t
\end{align}
%TODO: convert from \approx to = with epsilon
which implies that the scheme solves the original problem statement.

\subsection*{The function $d$}\label{sec:inner-decoder}
The function $d:\mathbb{N}\rightarrow\mathbb{N}\times\mathcal{A}$ must be a
bijection, and we aim for $d$ to satisfy \cref{eq:inner-decoder}, and thus
$P(x)\approx \frac{s'}{s}$.  Achieving this is actually fairly straightforward.
One way to define a bijection $d:s\mapsto(s', x)$ is to start with a mapping
$\tilde d: s\mapsto x$, with the property that none of the preimages $\tilde
d^{-1}(x):=\{n\in\mathbb{N}:\tilde d(n) = x\}$ are finite for
$x\in\mathcal{A}$. Then let $s'$ be the index of $s$ within the (ordered) set
$\tilde d^{-1}(x)$, with indices starting at $0$. Equivalently, $s'$ is the
number of integers $n$ with $0\leq n<s$ and $d(n) = x$.
% TODO: Maybe explain explicitly why this is a guaranteed bijection

With this setup, the ratio
\begin{equation}\label{eq:ratio}
  \frac{s'}{s} = \frac{\abs{\{n\in\mathbb{N}: n < s, d(n) = x\}}}{s}
\end{equation}
is the density of numbers which decode to $x$, within all the natural numbers
less $s$. For large $s$ we can ensure that this ratio is close to $P(x)$ by
setting $\tilde d$ such that numbers which decode to a symbol $x$ are
distributed \emph{within the natural numbers} with density close to $P(x)$.

To do this, we partition $\mathbb{N}$ into finite ranges of equal length, and
treat each range as a model for the interval $[0, 1]$, with sub-intervals
within $[0, 1]$ corresponding to each symbol, and the width of each
sub-interval being equal to the corresponding symbol's probability (see
\cref{fig:interval}).  More precisely, we assume that probabilities $p_1,
\ldots, p_I$ are quantized to some precision $r_p$, i.e.  that there exist
$m_i\in\{1,\ldots,2^{r_p}\}$ such that $p_i = \frac{m_i}{2^{r_p}}$ for each
$i=1,\ldots,I$. This may require rounding of $p_i$, but for large enough $r_p$
the effect of this is negligable. The mapping $\tilde d$ can then be expressed
as a composition $\tilde d = \tilde d_2 \circ \tilde d_1$, where $\tilde d_1$
does the partitioning described above, and $\tilde d_2$ assigns numbers within
each partition to symbols. Formally
\begin{equation}
  \tilde d_1(s) := s \bmod 2^{r_p}.
\end{equation}
Using the shorthand $\bar{s} := \tilde d_1 (a)$, and letting $c_i :=
\sum_{j=1}^i m_j$ denote the (quantized) cumulative probability of symbol
$a_i$,
\begin{equation}
  \tilde d_2(\bar s) := a_i\text{ where }i := \max \{j : c_j \leq \bar s\}.
\end{equation}
\Cref{fig:interval} illustrates this mapping, with a particular probability
distribution, for the range $s = 64,\ldots, 71$.

\begin{figure}[h]
  \centering
  \drawinterval
  \caption{
    Showing the correspondence between $s$, $s \bmod 2^{r_p}$ and the symbol
    $x$. The interval $[0, 1]\subset\mathbb{R}$ is modelled by the set of
    integers $\{0, 1, \ldots, 2^{r_p} - 1\}$. In this case $r_p = 3$ and the
    probabilities of each symbol are $P(a) = \frac{1}{8}$, $P(b) =
    \frac{2}{8}$, $P(c) = \frac{3}{8}$ and $P(d) = \frac{1}{8}$.}
  \label{fig:interval}
\end{figure}

\begin{figure}[h]
  \centering
  \drawpmf \quad \drawapprox{20}{\small} \quad \drawapprox{70}{\tiny}
  \caption{
    Showing the pmf of a distribution over symbols (left) and a visualization
    of the mapping $d$ (middle and right).  Numbers less than or equal to $s$
    are shown in bold, for $s=20$ and $s=70$ (middle and right, respecitively),
    and the ratio in \cref{eq:ratio} can be seen to approach the histogram in
    the left hand plot.}\label{fig:visual-ans}
\end{figure}

\subsection*{Computing $s'$}
The number $s'$ was defined above as `the index of $s$ within the (ordered) set
$\tilde d^{-1}(x)$, with indices starting at $0$'. We now derive an expression
for $s'$ in terms of $s$, $m_i$ and $c_i$, where $i = \max\{j: c_j\leq \bar
s\}$ (as above), and we formalise \cref{eq:inner-decoder}.

Our expression for $s'$ is a sum of two terms. The first term counts the entire
intervals, corresponding to $a_i$, which are below $s$. This is equal to $m_i
\cdot (s \div 2^{r_p})$, where $\div$ denotes \emph{integer} division,
discarding any remainder. The second term counts our position within the
current inverval, which is $\bar s - c_i \equiv s\bmod 2^{r_p} - c_i$. Thus
\begin{equation}
  s' = m_i \cdot (s \div 2^{r_p}) + s\bmod 2^{r_p} - c_i.
\end{equation}
This expression is straightforward to compute. Moreover it is straightforward
to show that the ratio $\frac{s'}{s}$ is bounded below:
\begin{align}
  \frac{s'}{s}
    &=    \frac{m_i\cdot(s \div 2^{r_p}) + s\bmod 2^{r_p} - c_i.}{s}\\
    &\geq \frac{m_i\cdot(s \div 2^{r_p})}{s}\\
    &\geq \frac{m_i\cdot\left(\frac{s}{2^{r_p}} - 1\right)}{s}\\
    &\geq \frac{m_i}{2^{r_p}} - \frac{1}{s}\\
    &\geq p_i - \frac{1}{2^{r_s - r_t}}
\end{align}
since $s\bmod 2^{r_p} > c_i$ by the definition of $i$, and using the fact that
$s\div 2^{r_p} = \frac{s - s\bmod 2^{r_p}}{2^{r_p}}$ by the definition of
$\div$ and $s\geq 2^{r_s - r_t}$. By choosing $r_s - r_t$ to be reasonably
large (it is equal to 32 in our implementation), we ensure that $\frac{s'}{s}$
is very close to $p_i$. This behaviour can be seen visually in
\cref{fig:visual-ans}, which shows the improvement in the approximation for
larger $s$.

\subsection*{Pseudocode for $d$}
We now have everything we need to write down a procedure to compute $d$. We
assume access to a function $f_P:\bar{s}\mapsto a_i, c_i, m_i$, where $i$ is
defined above. This function clearly depends on the distribution $P$, and its
computational complexity is equivalent to that of computing the CDF and inverse
CDF of $P$. For many common distributions, the CDF and inverse CDF have
straightforward closed form expressions.

We compute $d$ as follows:
\begin{lstlisting}
def $d$($s$):
    $\bar s$ = $s\bmod 2^{r_p}$
    $x$, $c_x$, $m_x$ = $f_P(\bar s)$
    $s'$ = $m_x \cdot (s \div 2^{r_p}) + \bar s - c_x$
    return $s'$, $x$
\end{lstlisting}

\subsection*{Inverting the decoder}
Having described a decoding process which appears not to throw away any
information, we now derive the inverse process and show that it is
computationally straightforward. On a high level, the inverse of the function
$D$ can be expressed as
\begin{lstlisting}
def $C$(($s$, $t$), $x$):
    $m$, $c$ = $g_P$($x$)
    $s$, $t$ = renorm_inverse($s$, $t$, $m$)
    $s$, $x$ = $d$($s$)
    return ($s$, $t$), $x$
\end{lstlisting}
\printbibliography
\end{document}
